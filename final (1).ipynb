{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce47dca-693b-4302-858f-b42f3d1a2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.54.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (7.4.4)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (6.30.0)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (4.4.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (1.8.15)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (27.0.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (6.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (2.2.6)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (63.2.0)\n",
      "Requirement already satisfied: tomli>=1.2.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab->jupyter) (2.2.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.22.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.15)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.26.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.21.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20250708)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\saheel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 799.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 799.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 799.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 799.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 799.2 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 551.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.8/2.2 MB 464.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 513.8 kB/s  0:00:03\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets, jupyter-console, jupyter\n",
      "\n",
      "   ---------------------------------------- 0/5 [widgetsnbextension]\n",
      "   -------- ------------------------------- 1/5 [jupyterlab_widgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ---------------- ----------------------- 2/5 [ipywidgets]\n",
      "   ------------------------ --------------- 3/5 [jupyter-console]\n",
      "   ------------------------ --------------- 3/5 [jupyter-console]\n",
      "   ------------------------ --------------- 3/5 [jupyter-console]\n",
      "   ------------------------ --------------- 3/5 [jupyter-console]\n",
      "   ------------------------ --------------- 3/5 [jupyter-console]\n",
      "   ---------------------------------------- 5/5 [jupyter]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "âœ“ All packages imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "# Run this once to install packages\n",
    "!pip install sentence-transformers scikit-learn lightgbm pandas numpy scipy\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lightgbm as lgb\n",
    "from scipy.optimize import minimize\n",
    "import re\n",
    "import warnings\n",
    "!pip install -U jupyter ipywidgets tqdm\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ“ All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "106ec4e4-ad53-4b80-afec-4b6a2c3d31eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "Columns: ['sample_id', 'catalog_content', 'image_link', 'price']\n",
      "\n",
      "Price statistics:\n",
      "count    75000.000000\n",
      "mean        23.647654\n",
      "std         33.376932\n",
      "min          0.130000\n",
      "25%          6.795000\n",
      "50%         14.000000\n",
      "75%         28.625000\n",
      "max       2796.000000\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your datasets\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# âœ… Assuming you have a submission DataFrame (rename 'df' to your actual variable)\n",
    "# Example: If your predictions are in a DataFrame named 'submission'\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'].astype(int),\n",
    "    'price': final_ensemble_test.astype(float).round(2)\n",
    "})\n",
    "\n",
    "# âœ… Save properly\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# âœ… Display some info\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nColumns: {train.columns.tolist()}\")\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(train['price'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6ad44f-7e4f-42bb-a05f-bfc0b540c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Text cleaning done!\n",
      "Sample: item name la victoria green taco sauce mild 12 ounce pack of 6 value 72 0 unit fl oz...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)  # Remove special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()      # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "train['catalog_clean'] = train['catalog_content'].apply(clean_text)\n",
    "test['catalog_clean'] = test['catalog_content'].apply(clean_text)\n",
    "\n",
    "print(\"âœ“ Text cleaning done!\")\n",
    "print(f\"Sample: {train['catalog_clean'].iloc[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c3a526-251a-4d87-9a43-e4a048d437f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Text features extracted: (75000, 8)\n",
      "   text_length  word_count  avg_word_length  num_digits  has_numbers  \\\n",
      "0           84          19         4.200000           6            1   \n",
      "1          491          81         5.987805          17            1   \n",
      "2          315          61         5.080645          12            1   \n",
      "3         1272         213         5.943925          17            1   \n",
      "4          142          29         4.733333          13            1   \n",
      "\n",
      "   ipq_extracted  has_premium  has_budget  \n",
      "0              1            0           1  \n",
      "1              1            0           1  \n",
      "2              1            0           1  \n",
      "3              1            1           1  \n",
      "4              1            0           1  \n"
     ]
    }
   ],
   "source": [
    "def extract_text_features(df):\n",
    "    \"\"\"Extract statistical features from text\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Length features\n",
    "    features['text_length'] = df['catalog_clean'].str.len()\n",
    "    features['word_count'] = df['catalog_clean'].str.split().str.len()\n",
    "    features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1)\n",
    "    \n",
    "    # Numeric features\n",
    "    features['num_digits'] = df['catalog_clean'].str.count(r'\\d')\n",
    "    features['has_numbers'] = (features['num_digits'] > 0).astype(int)\n",
    "    \n",
    "    # Extract quantity/IPQ\n",
    "    def extract_quantity(text):\n",
    "        matches = re.findall(r'(\\d+)\\s*(?:pack|pcs|pieces|count|qty)', str(text))\n",
    "        return int(matches[0]) if matches else 1\n",
    "    \n",
    "    features['ipq_extracted'] = df['catalog_content'].apply(extract_quantity)\n",
    "    \n",
    "    # Premium/budget indicators\n",
    "    features['has_premium'] = df['catalog_clean'].str.contains(\n",
    "        'premium|luxury|deluxe|professional|pro', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_budget'] = df['catalog_clean'].str.contains(\n",
    "        'budget|economy|basic|value', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "train_text_features = extract_text_features(train)\n",
    "test_text_features = extract_text_features(test)\n",
    "\n",
    "print(f\"âœ“ Text features extracted: {train_text_features.shape}\")\n",
    "print(train_text_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3fcb65-b468-4890-84ca-3956b445825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1/3: Loading all-MiniLM-L6-v2...\n",
      "âœ“ Model loaded successfully!\n",
      "\n",
      "ðŸ”„ Generating TRAIN embeddings...\n",
      "Processed 0/75000 texts\n",
      "Processed 2560/75000 texts\n",
      "Processed 5120/75000 texts\n",
      "Processed 7680/75000 texts\n",
      "Processed 10240/75000 texts\n",
      "Processed 12800/75000 texts\n",
      "Processed 15360/75000 texts\n",
      "Processed 17920/75000 texts\n",
      "Processed 20480/75000 texts\n",
      "Processed 23040/75000 texts\n",
      "Processed 25600/75000 texts\n",
      "Processed 28160/75000 texts\n",
      "Processed 30720/75000 texts\n",
      "Processed 33280/75000 texts\n",
      "Processed 35840/75000 texts\n",
      "Processed 38400/75000 texts\n",
      "Processed 40960/75000 texts\n",
      "Processed 43520/75000 texts\n",
      "Processed 46080/75000 texts\n",
      "Processed 48640/75000 texts\n",
      "Processed 51200/75000 texts\n",
      "Processed 53760/75000 texts\n",
      "Processed 56320/75000 texts\n",
      "Processed 58880/75000 texts\n",
      "Processed 61440/75000 texts\n",
      "Processed 64000/75000 texts\n",
      "Processed 66560/75000 texts\n",
      "Processed 69120/75000 texts\n",
      "Processed 71680/75000 texts\n",
      "Processed 74240/75000 texts\n",
      "âœ“ Train embeddings: (75000, 384)\n",
      "\n",
      "ðŸ”„ Generating TEST embeddings...\n",
      "Processed 0/75000 texts\n",
      "Processed 2560/75000 texts\n",
      "Processed 5120/75000 texts\n",
      "Processed 7680/75000 texts\n",
      "Processed 10240/75000 texts\n",
      "Processed 12800/75000 texts\n",
      "Processed 15360/75000 texts\n",
      "Processed 17920/75000 texts\n",
      "Processed 20480/75000 texts\n",
      "Processed 23040/75000 texts\n",
      "Processed 25600/75000 texts\n",
      "Processed 28160/75000 texts\n",
      "Processed 30720/75000 texts\n",
      "Processed 33280/75000 texts\n",
      "Processed 35840/75000 texts\n",
      "Processed 38400/75000 texts\n",
      "Processed 40960/75000 texts\n",
      "Processed 43520/75000 texts\n",
      "Processed 46080/75000 texts\n",
      "Processed 48640/75000 texts\n",
      "Processed 51200/75000 texts\n",
      "Processed 53760/75000 texts\n",
      "Processed 56320/75000 texts\n",
      "Processed 58880/75000 texts\n",
      "Processed 61440/75000 texts\n",
      "Processed 64000/75000 texts\n",
      "Processed 66560/75000 texts\n",
      "Processed 69120/75000 texts\n",
      "Processed 71680/75000 texts\n",
      "Processed 74240/75000 texts\n",
      "âœ“ Test embeddings: (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def load_model_with_retry(model_name, max_retries=3):\n",
    "    \"\"\"Load model with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries}: Loading {model_name}...\")\n",
    "            model = SentenceTransformer(model_name)\n",
    "            print(f\"âœ“ Model loaded successfully!\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 5 * (attempt + 1)\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# Load model with retry\n",
    "embedding_model = load_model_with_retry('all-MiniLM-L6-v2', max_retries=3)\n",
    "\n",
    "def get_embeddings(texts, batch_size=256):\n",
    "    \"\"\"Generate embeddings in batches\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        batch_embeddings = embedding_model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {i}/{len(texts)} texts\")\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"\\nðŸ”„ Generating TRAIN embeddings...\")\n",
    "train_embeddings = get_embeddings(train['catalog_clean'])\n",
    "print(f\"âœ“ Train embeddings: {train_embeddings.shape}\")\n",
    "\n",
    "print(\"\\nðŸ”„ Generating TEST embeddings...\")\n",
    "test_embeddings = get_embeddings(test['catalog_clean'])\n",
    "print(f\"âœ“ Test embeddings: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5be2e0-1783-4555-bb38-aec6e99dfece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SVD to reduce dimensions...\n",
      "âœ“ Reduced embeddings shape: (75000, 128)\n",
      "âœ“ Explained variance: 0.8679\n"
     ]
    }
   ],
   "source": [
    "# Reduce from 384 to 128 dimensions for faster training\n",
    "print(\"Applying SVD to reduce dimensions...\")\n",
    "svd = TruncatedSVD(n_components=128, random_state=42)\n",
    "\n",
    "train_embeddings_reduced = svd.fit_transform(train_embeddings)\n",
    "test_embeddings_reduced = svd.transform(test_embeddings)\n",
    "\n",
    "print(f\"âœ“ Reduced embeddings shape: {train_embeddings_reduced.shape}\")\n",
    "print(f\"âœ“ Explained variance: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Update embeddings\n",
    "train_embeddings = train_embeddings_reduced\n",
    "test_embeddings = test_embeddings_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015f7da2-2b3b-4c2b-af45-68462223f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "âœ“ TF-IDF shape: (75000, 100)\n",
      "âœ“ TF-IDF variance explained: 0.2712\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train['catalog_clean'])\n",
    "test_tfidf = tfidf.transform(test['catalog_clean'])\n",
    "\n",
    "# Apply SVD to reduce TF-IDF dimensions\n",
    "tfidf_svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "train_tfidf_dense = tfidf_svd.fit_transform(train_tfidf)\n",
    "test_tfidf_dense = tfidf_svd.transform(test_tfidf)\n",
    "\n",
    "print(f\"âœ“ TF-IDF shape: {train_tfidf_dense.shape}\")\n",
    "print(f\"âœ“ TF-IDF variance explained: {tfidf_svd.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cef8d25-4d60-48a2-82e3-705be262dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Combined feature matrix:\n",
      "  Train: (75000, 236)\n",
      "  Test: (75000, 236)\n",
      "âœ“ Features scaled and ready!\n"
     ]
    }
   ],
   "source": [
    "# Stack all features together\n",
    "X_train = np.hstack([\n",
    "    train_embeddings,           # 128 dims - semantic meaning\n",
    "    train_tfidf_dense,          # 100 dims - keyword features\n",
    "    train_text_features.values  # 8 dims - statistical features\n",
    "])\n",
    "\n",
    "X_test = np.hstack([\n",
    "    test_embeddings,\n",
    "    test_tfidf_dense,\n",
    "    test_text_features.values\n",
    "])\n",
    "\n",
    "print(f\"âœ“ Combined feature matrix:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "\n",
    "# Scale features (important for Ridge/Lasso)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Prepare target variable\n",
    "y_train = train['price'].values\n",
    "y_train_log = np.log1p(y_train)  # Log transformation\n",
    "\n",
    "print(f\"âœ“ Features scaled and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53cdac9-62ef-4408-9b71-2bc846ca23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SMAPE function:\n",
      "SMAPE: 7.03%\n",
      "âœ“ SMAPE function ready!\n"
     ]
    }
   ],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    # Avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "    smape_val = 200 * np.mean(diff / denominator)\n",
    "    return smape_val\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing SMAPE function:\")\n",
    "test_true = np.array([100, 200, 300])\n",
    "test_pred = np.array([110, 190, 320])\n",
    "print(f\"SMAPE: {smape(test_true, test_pred):.2f}%\")\n",
    "print(\"âœ“ SMAPE function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92026adb-b622-478a-8e0b-4c5887433c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ 5-Fold Cross-Validation setup complete!\n",
      "Training on 75000 samples\n"
     ]
    }
   ],
   "source": [
    "# Setup cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for predictions\n",
    "oof_predictions = {}  # Out-of-fold predictions\n",
    "test_predictions = {}  # Test predictions\n",
    "\n",
    "print(f\"âœ“ {n_splits}-Fold Cross-Validation setup complete!\")\n",
    "print(f\"Training on {len(X_train_scaled)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec1e55d-cc9a-4157-b7a1-fc993f8dc494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING RIDGE REGRESSION\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 SMAPE: 61.93%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 SMAPE: 61.24%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 SMAPE: 61.35%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 SMAPE: 60.31%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 SMAPE: 61.27%\n",
      "\n",
      "============================================================\n",
      "âœ“ RIDGE Overall SMAPE: 61.22%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RIDGE REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ridge_oof = np.zeros(len(X_train_scaled))\n",
    "ridge_test = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    # Train Ridge\n",
    "    ridge = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    ridge_oof[val_idx] = ridge.predict(X_val)\n",
    "    ridge_test += ridge.predict(X_test_scaled) / n_splits\n",
    "    \n",
    "    # Calculate SMAPE for this fold\n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(ridge_oof[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions['ridge'] = np.expm1(ridge_oof)\n",
    "test_predictions['ridge'] = np.expm1(ridge_test)\n",
    "\n",
    "overall_smape = smape(y_train, oof_predictions['ridge'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ RIDGE Overall SMAPE: {overall_smape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02db811-de15-4fd7-9034-51511dc2f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LASSO REGRESSION\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 SMAPE: 69.21%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 SMAPE: 68.74%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 SMAPE: 68.67%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 SMAPE: 67.91%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 SMAPE: 68.88%\n",
      "\n",
      "============================================================\n",
      "âœ“ LASSO Overall SMAPE: 68.68%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LASSO REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lasso_oof = np.zeros(len(X_train_scaled))\n",
    "lasso_test = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    # Train Lasso\n",
    "    lasso = Lasso(alpha=0.1, random_state=42, max_iter=2000)\n",
    "    lasso.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    lasso_oof[val_idx] = lasso.predict(X_val)\n",
    "    lasso_test += lasso.predict(X_test_scaled) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(lasso_oof[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions['lasso'] = np.expm1(lasso_oof)\n",
    "test_predictions['lasso'] = np.expm1(lasso_test)\n",
    "\n",
    "overall_smape = smape(y_train, oof_predictions['lasso'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ LASSO Overall SMAPE: {overall_smape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722b34c5-1c3c-46c8-a565-4c97893fcaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING ELASTICNET\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 SMAPE: 73.29%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 SMAPE: 72.67%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 SMAPE: 72.66%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 SMAPE: 72.03%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 SMAPE: 72.82%\n",
      "\n",
      "============================================================\n",
      "âœ“ ELASTICNET Overall SMAPE: 72.69%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ELASTICNET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "elastic_oof = np.zeros(len(X_train_scaled))\n",
    "elastic_test = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    # Train ElasticNet\n",
    "    elastic = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "    elastic.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    elastic_oof[val_idx] = elastic.predict(X_val)\n",
    "    elastic_test += elastic.predict(X_test_scaled) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(elastic_oof[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "oof_predictions['elastic'] = np.expm1(elastic_oof)\n",
    "test_predictions['elastic'] = np.expm1(elastic_test)\n",
    "\n",
    "overall_smape = smape(y_train, oof_predictions['elastic'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ ELASTICNET Overall SMAPE: {overall_smape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0649a0-b754-47a5-9069-d906a8ab7ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LIGHTGBM\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.585838\n",
      "[400]\tvalid_0's l1: 0.572277\n",
      "[600]\tvalid_0's l1: 0.566795\n",
      "[800]\tvalid_0's l1: 0.562834\n",
      "[1000]\tvalid_0's l1: 0.559344\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's l1: 0.559328\n",
      "Fold 1 SMAPE: 55.89%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.576915\n",
      "[400]\tvalid_0's l1: 0.564464\n",
      "[600]\tvalid_0's l1: 0.558589\n",
      "[800]\tvalid_0's l1: 0.55542\n",
      "[1000]\tvalid_0's l1: 0.552087\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 0.552087\n",
      "Fold 2 SMAPE: 55.29%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.580389\n",
      "[400]\tvalid_0's l1: 0.566644\n",
      "[600]\tvalid_0's l1: 0.560535\n",
      "[800]\tvalid_0's l1: 0.556155\n",
      "[1000]\tvalid_0's l1: 0.55338\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's l1: 0.553355\n",
      "Fold 3 SMAPE: 55.53%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.570359\n",
      "[400]\tvalid_0's l1: 0.558741\n",
      "[600]\tvalid_0's l1: 0.553497\n",
      "[800]\tvalid_0's l1: 0.54951\n",
      "[1000]\tvalid_0's l1: 0.54691\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's l1: 0.546908\n",
      "Fold 4 SMAPE: 54.78%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 0.583883\n",
      "[400]\tvalid_0's l1: 0.571174\n",
      "[600]\tvalid_0's l1: 0.565564\n",
      "[800]\tvalid_0's l1: 0.561734\n",
      "[1000]\tvalid_0's l1: 0.559157\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l1: 0.559157\n",
      "Fold 5 SMAPE: 55.76%\n",
      "\n",
      "============================================================\n",
      "âœ“ LIGHTGBM Overall SMAPE: 55.45%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgb_oof = np.zeros(len(X_train_scaled))\n",
    "lgb_test = np.zeros(len(X_test_scaled))\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 64,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(200)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lgb_oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    lgb_test += model.predict(X_test_scaled, num_iteration=model.best_iteration) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(lgb_oof[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "oof_predictions['lgbm'] = np.expm1(lgb_oof)\n",
    "test_predictions['lgbm'] = np.expm1(lgb_test)\n",
    "\n",
    "overall_smape = smape(y_train, oof_predictions['lgbm'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ LIGHTGBM Overall SMAPE: {overall_smape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de36ec7c-a129-4173-a351-74cf5ce2a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "RIDGE                SMAPE: 61.22%\n",
      "LASSO                SMAPE: 68.68%\n",
      "ELASTIC              SMAPE: 72.69%\n",
      "LGBM                 SMAPE: 55.45%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, predictions in oof_predictions.items():\n",
    "    score = smape(y_train, predictions)\n",
    "    print(f\"{model_name.upper():20s} SMAPE: {score:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd8dc4a-9ba7-49af-b563-fd44bb44d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OPTIMIZING ENSEMBLE WEIGHTS\n",
      "======================================================================\n",
      "Models to ensemble: ['ridge', 'lasso', 'elastic', 'lgbm']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 55.448095\n",
      "         Iterations: 109\n",
      "         Function evaluations: 191\n",
      "\n",
      "======================================================================\n",
      "OPTIMAL ENSEMBLE WEIGHTS:\n",
      "======================================================================\n",
      "RIDGE                weight: 0.0000 (0.0%)\n",
      "LASSO                weight: 0.0000 (0.0%)\n",
      "ELASTIC              weight: 0.0000 (0.0%)\n",
      "LGBM                 weight: 1.0000 (100.0%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZING ENSEMBLE WEIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def ensemble_smape_objective(weights, predictions_dict, y_true):\n",
    "    \"\"\"Objective function for ensemble optimization\"\"\"\n",
    "    weights = np.abs(weights)  # Ensure positive\n",
    "    weights /= weights.sum()    # Normalize to sum=1\n",
    "    \n",
    "    # Weighted ensemble prediction\n",
    "    ensemble_pred = np.zeros(len(y_true))\n",
    "    for i, (name, pred) in enumerate(predictions_dict.items()):\n",
    "        ensemble_pred += weights[i] * pred\n",
    "    \n",
    "    return smape(y_true, ensemble_pred)\n",
    "\n",
    "# Get model names\n",
    "model_names = list(oof_predictions.keys())\n",
    "print(f\"Models to ensemble: {model_names}\")\n",
    "\n",
    "# Initial weights (equal)\n",
    "initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "\n",
    "# Optimize weights using Nelder-Mead\n",
    "result = minimize(\n",
    "    ensemble_smape_objective,\n",
    "    initial_weights,\n",
    "    args=(oof_predictions, y_train),\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 1000, 'disp': True}\n",
    ")\n",
    "\n",
    "# Get optimal weights\n",
    "optimal_weights = np.abs(result.x)\n",
    "optimal_weights /= optimal_weights.sum()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OPTIMAL ENSEMBLE WEIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "for name, weight in zip(model_names, optimal_weights):\n",
    "    print(f\"{name.upper():20s} weight: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eab7e44-7975-44a7-b617-f1cbbe300968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "âœ“ Ensemble OOF SMAPE: 55.45%\n",
      "======================================================================\n",
      "\n",
      "COMPARISON:\n",
      "  RIDGE               : 61.22%\n",
      "  LASSO               : 68.68%\n",
      "  ELASTIC             : 72.69%\n",
      "  LGBM                : 55.45%\n",
      "  ENSEMBLE            : 55.45%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create weighted ensemble predictions\n",
    "ensemble_oof = np.zeros(len(y_train))\n",
    "ensemble_test = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for i, (name, pred) in enumerate(oof_predictions.items()):\n",
    "    ensemble_oof += optimal_weights[i] * pred\n",
    "    ensemble_test += optimal_weights[i] * test_predictions[name]\n",
    "\n",
    "ensemble_smape_score = smape(y_train, ensemble_oof)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ Ensemble OOF SMAPE: {ensemble_smape_score:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nCOMPARISON:\")\n",
    "for name, pred in oof_predictions.items():\n",
    "    score = smape(y_train, pred)\n",
    "    print(f\"  {name.upper():20s}: {score:.2f}%\")\n",
    "print(f\"  {'ENSEMBLE':20s}: {ensemble_smape_score:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df9b7294-7bf5-48a8-a02a-936bc13c6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing applied:\n",
      "  Original range: [1.19, 229.07]\n",
      "  Clipped range:  [1.19, 229.07]\n",
      "  Training range: [0.13, 2796.00]\n"
     ]
    }
   ],
   "source": [
    "# Apply smart post-processing\n",
    "def postprocess_predictions(predictions, train_prices):\n",
    "    \"\"\"Clip extreme predictions to realistic ranges\"\"\"\n",
    "    \n",
    "    # Get percentile bounds from training data\n",
    "    lower_bound = train_prices.quantile(0.001)\n",
    "    upper_bound = train_prices.quantile(0.999)\n",
    "    \n",
    "    # Clip predictions\n",
    "    predictions_clipped = np.clip(predictions, lower_bound, upper_bound)\n",
    "    \n",
    "    # Ensure all positive\n",
    "    predictions_clipped = np.maximum(predictions_clipped, 0.01)\n",
    "    \n",
    "    return predictions_clipped\n",
    "\n",
    "# Apply post-processing\n",
    "ensemble_test_final = postprocess_predictions(ensemble_test, train['price'])\n",
    "\n",
    "print(\"Post-processing applied:\")\n",
    "print(f\"  Original range: [{ensemble_test.min():.2f}, {ensemble_test.max():.2f}]\")\n",
    "print(f\"  Clipped range:  [{ensemble_test_final.min():.2f}, {ensemble_test_final.max():.2f}]\")\n",
    "print(f\"  Training range: [{train['price'].min():.2f}, {train['price'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cbfe321-4451-42ef-9a41-b4e9e6e982b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING SENTENCE EMBEDDINGS WITH DOWNLOADED MODEL\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Generating TRAIN embeddings...\n",
      "  Processed 0/75000 texts (0.0%)\n",
      "  Processed 2560/75000 texts (3.4%)\n",
      "  Processed 5120/75000 texts (6.8%)\n",
      "  Processed 7680/75000 texts (10.2%)\n",
      "  Processed 10240/75000 texts (13.7%)\n",
      "  Processed 12800/75000 texts (17.1%)\n",
      "  Processed 15360/75000 texts (20.5%)\n",
      "  Processed 17920/75000 texts (23.9%)\n",
      "  Processed 20480/75000 texts (27.3%)\n",
      "  Processed 23040/75000 texts (30.7%)\n",
      "  Processed 25600/75000 texts (34.1%)\n",
      "  Processed 28160/75000 texts (37.5%)\n",
      "  Processed 30720/75000 texts (41.0%)\n",
      "  Processed 33280/75000 texts (44.4%)\n",
      "  Processed 35840/75000 texts (47.8%)\n",
      "  Processed 38400/75000 texts (51.2%)\n",
      "  Processed 40960/75000 texts (54.6%)\n",
      "  Processed 43520/75000 texts (58.0%)\n",
      "  Processed 46080/75000 texts (61.4%)\n",
      "  Processed 48640/75000 texts (64.9%)\n",
      "  Processed 51200/75000 texts (68.3%)\n",
      "  Processed 53760/75000 texts (71.7%)\n",
      "  Processed 56320/75000 texts (75.1%)\n",
      "  Processed 58880/75000 texts (78.5%)\n",
      "  Processed 61440/75000 texts (81.9%)\n",
      "  Processed 64000/75000 texts (85.3%)\n",
      "  Processed 66560/75000 texts (88.7%)\n",
      "  Processed 69120/75000 texts (92.2%)\n",
      "  Processed 71680/75000 texts (95.6%)\n",
      "  Processed 74240/75000 texts (99.0%)\n",
      "âœ“ Train embeddings: (75000, 384)\n",
      "\n",
      "ðŸ”„ Generating TEST embeddings...\n",
      "  Processed 0/75000 texts (0.0%)\n",
      "  Processed 2560/75000 texts (3.4%)\n",
      "  Processed 5120/75000 texts (6.8%)\n",
      "  Processed 7680/75000 texts (10.2%)\n",
      "  Processed 10240/75000 texts (13.7%)\n",
      "  Processed 12800/75000 texts (17.1%)\n",
      "  Processed 15360/75000 texts (20.5%)\n",
      "  Processed 17920/75000 texts (23.9%)\n",
      "  Processed 20480/75000 texts (27.3%)\n",
      "  Processed 23040/75000 texts (30.7%)\n",
      "  Processed 25600/75000 texts (34.1%)\n",
      "  Processed 28160/75000 texts (37.5%)\n",
      "  Processed 30720/75000 texts (41.0%)\n",
      "  Processed 33280/75000 texts (44.4%)\n",
      "  Processed 35840/75000 texts (47.8%)\n",
      "  Processed 38400/75000 texts (51.2%)\n",
      "  Processed 40960/75000 texts (54.6%)\n",
      "  Processed 43520/75000 texts (58.0%)\n",
      "  Processed 46080/75000 texts (61.4%)\n",
      "  Processed 48640/75000 texts (64.9%)\n",
      "  Processed 51200/75000 texts (68.3%)\n",
      "  Processed 53760/75000 texts (71.7%)\n",
      "  Processed 56320/75000 texts (75.1%)\n",
      "  Processed 58880/75000 texts (78.5%)\n",
      "  Processed 61440/75000 texts (81.9%)\n",
      "  Processed 64000/75000 texts (85.3%)\n",
      "  Processed 66560/75000 texts (88.7%)\n",
      "  Processed 69120/75000 texts (92.2%)\n",
      "  Processed 71680/75000 texts (95.6%)\n",
      "  Processed 74240/75000 texts (99.0%)\n",
      "âœ“ Test embeddings: (75000, 384)\n",
      "\n",
      "Applying SVD dimensionality reduction...\n",
      "âœ“ Reduced embeddings: (75000, 200)\n",
      "âœ“ Variance explained: 0.9532\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING SENTENCE EMBEDDINGS WITH DOWNLOADED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_embeddings_batch(texts, model, batch_size=256):\n",
    "    \"\"\"Generate embeddings in batches\"\"\"\n",
    "    embeddings = []\n",
    "    total = len(texts)\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(\n",
    "            batch, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"  Processed {i}/{total} texts ({i/total*100:.1f}%)\")\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"\\nðŸ”„ Generating TRAIN embeddings...\")\n",
    "train_embeddings_real = get_embeddings_batch(train['catalog_clean'], embedding_model)\n",
    "print(f\"âœ“ Train embeddings: {train_embeddings_real.shape}\")\n",
    "\n",
    "print(\"\\nðŸ”„ Generating TEST embeddings...\")\n",
    "test_embeddings_real = get_embeddings_batch(test['catalog_clean'], embedding_model)\n",
    "print(f\"âœ“ Test embeddings: {test_embeddings_real.shape}\")\n",
    "\n",
    "# Reduce dimensions with SVD\n",
    "print(\"\\nApplying SVD dimensionality reduction...\")\n",
    "svd_embeddings = TruncatedSVD(n_components=200, random_state=42)\n",
    "train_embeddings_reduced = svd_embeddings.fit_transform(train_embeddings_real)\n",
    "test_embeddings_reduced = svd_embeddings.transform(test_embeddings_real)\n",
    "\n",
    "print(f\"âœ“ Reduced embeddings: {train_embeddings_reduced.shape}\")\n",
    "print(f\"âœ“ Variance explained: {svd_embeddings.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8e15abb-9c66-472d-8f3c-c7e57517ff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING ENHANCED TF-IDF FEATURES\n",
      "======================================================================\n",
      "Fitting word TF-IDF...\n",
      "Fitting char TF-IDF...\n",
      "âœ“ Word TF-IDF: (75000, 150)\n",
      "âœ“ Char TF-IDF: (75000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING ENHANCED TF-IDF FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Multiple TF-IDF with different n-gram ranges\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=3,\n",
    "    analyzer='char',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"Fitting word TF-IDF...\")\n",
    "train_tfidf_w = tfidf_word.fit_transform(train['catalog_clean'])\n",
    "test_tfidf_w = tfidf_word.transform(test['catalog_clean'])\n",
    "\n",
    "print(\"Fitting char TF-IDF...\")\n",
    "train_tfidf_c = tfidf_char.fit_transform(train['catalog_clean'])\n",
    "test_tfidf_c = tfidf_char.transform(test['catalog_clean'])\n",
    "\n",
    "# SVD reduction\n",
    "svd_word = TruncatedSVD(n_components=150, random_state=42)\n",
    "train_word_svd = svd_word.fit_transform(train_tfidf_w)\n",
    "test_word_svd = svd_word.transform(test_tfidf_w)\n",
    "\n",
    "svd_char = TruncatedSVD(n_components=100, random_state=42)\n",
    "train_char_svd = svd_char.fit_transform(train_tfidf_c)\n",
    "test_char_svd = svd_char.transform(test_tfidf_c)\n",
    "\n",
    "print(f\"âœ“ Word TF-IDF: {train_word_svd.shape}\")\n",
    "print(f\"âœ“ Char TF-IDF: {train_char_svd.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1eb23186-4678-4899-ab83-69e82dea8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting advanced features...\n",
      "âœ“ Advanced features: (75000, 14)\n"
     ]
    }
   ],
   "source": [
    "def extract_advanced_features(df):\n",
    "    \"\"\"Extract comprehensive features\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Text statistics\n",
    "    features['text_length'] = df['catalog_clean'].str.len()\n",
    "    features['word_count'] = df['catalog_clean'].str.split().str.len()\n",
    "    features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1)\n",
    "    features['unique_word_ratio'] = df['catalog_clean'].apply(\n",
    "        lambda x: len(set(str(x).split())) / (len(str(x).split()) + 1)\n",
    "    )\n",
    "    \n",
    "    # Numeric features\n",
    "    features['num_count'] = df['catalog_clean'].str.count(r'\\d')\n",
    "    features['has_numbers'] = (features['num_count'] > 0).astype(int)\n",
    "    features['number_density'] = features['num_count'] / (features['text_length'] + 1)\n",
    "    \n",
    "    # Extract IPQ\n",
    "    def extract_quantity(text):\n",
    "        matches = re.findall(r'(\\d+)\\s*(?:pack|pcs|pieces|count|qty|units?)', str(text).lower())\n",
    "        return int(matches[0]) if matches else 1\n",
    "    \n",
    "    features['ipq'] = df['catalog_content'].apply(extract_quantity)\n",
    "    features['ipq_log'] = np.log1p(features['ipq'])\n",
    "    \n",
    "    # Premium/Budget indicators\n",
    "    features['has_premium'] = df['catalog_clean'].str.contains(\n",
    "        'premium|luxury|deluxe|professional|pro|elite', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_budget'] = df['catalog_clean'].str.contains(\n",
    "        'budget|economy|basic|value|cheap', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Material/Size/Brand\n",
    "    features['has_material'] = df['catalog_clean'].str.contains(\n",
    "        'cotton|plastic|metal|wood|steel|leather', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_size'] = df['catalog_clean'].str.contains(\n",
    "        'small|medium|large|xl|size|cm|inch|kg|liter', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_brand'] = df['catalog_clean'].str.contains(\n",
    "        'brand|original|authentic|genuine', na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extracting advanced features...\")\n",
    "train_adv = extract_advanced_features(train)\n",
    "test_adv = extract_advanced_features(test)\n",
    "print(f\"âœ“ Advanced features: {train_adv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43c7043a-5390-46a2-9c65-6a274db774b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMBINING ALL FEATURES - MAXIMUM FEATURE SET\n",
      "======================================================================\n",
      "Combined features: (75000, 464)\n",
      "âœ“ Final feature matrix: (75000, 464)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMBINING ALL FEATURES - MAXIMUM FEATURE SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train_ultimate = np.hstack([\n",
    "    train_embeddings_reduced,  # 200 - semantic embeddings\n",
    "    train_word_svd,            # 150 - word n-grams\n",
    "    train_char_svd,            # 100 - char n-grams\n",
    "    train_adv.values           # 14 - advanced features\n",
    "])\n",
    "\n",
    "X_test_ultimate = np.hstack([\n",
    "    test_embeddings_reduced,\n",
    "    test_word_svd,\n",
    "    test_char_svd,\n",
    "    test_adv.values\n",
    "])\n",
    "\n",
    "print(f\"Combined features: {X_train_ultimate.shape}\")\n",
    "\n",
    "# Scale\n",
    "scaler_ultimate = StandardScaler()\n",
    "X_train_ultimate_scaled = scaler_ultimate.fit_transform(X_train_ultimate)\n",
    "X_test_ultimate_scaled = scaler_ultimate.transform(X_test_ultimate)\n",
    "\n",
    "print(f\"âœ“ Final feature matrix: {X_train_ultimate_scaled.shape}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0728f97b-b7d1-457b-b265-5e2125b110fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING RIDGE WITH ULTIMATE FEATURES\n",
      "======================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 SMAPE: 60.41%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 SMAPE: 59.43%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 SMAPE: 59.70%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 SMAPE: 58.72%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 SMAPE: 59.51%\n",
      "\n",
      "======================================================================\n",
      "âœ“ RIDGE FINAL SMAPE: 59.55%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING RIDGE WITH ULTIMATE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ridge_oof_final = np.zeros(len(X_train_ultimate_scaled))\n",
    "ridge_test_final = np.zeros(len(X_test_ultimate_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_ultimate_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_ultimate_scaled[train_idx], X_train_ultimate_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    ridge = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    \n",
    "    ridge_oof_final[val_idx] = ridge.predict(X_val)\n",
    "    ridge_test_final += ridge.predict(X_test_ultimate_scaled) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(ridge_oof_final[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "ridge_final_pred = np.expm1(ridge_oof_final)\n",
    "ridge_final_test = np.expm1(ridge_test_final)\n",
    "\n",
    "overall_ridge = smape(y_train, ridge_final_pred)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ RIDGE FINAL SMAPE: {overall_ridge:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ecaddbb-4637-4d47-85d2-12f3a16f4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING LASSO WITH ULTIMATE FEATURES\n",
      "======================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 SMAPE: 66.95%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 SMAPE: 66.45%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 SMAPE: 66.40%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 SMAPE: 65.59%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 SMAPE: 66.71%\n",
      "\n",
      "======================================================================\n",
      "âœ“ LASSO FINAL SMAPE: 66.42%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING LASSO WITH ULTIMATE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lasso_oof_final = np.zeros(len(X_train_ultimate_scaled))\n",
    "lasso_test_final = np.zeros(len(X_test_ultimate_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_ultimate_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_ultimate_scaled[train_idx], X_train_ultimate_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    lasso = Lasso(alpha=0.05, random_state=42, max_iter=3000)\n",
    "    lasso.fit(X_tr, y_tr)\n",
    "    \n",
    "    lasso_oof_final[val_idx] = lasso.predict(X_val)\n",
    "    lasso_test_final += lasso.predict(X_test_ultimate_scaled) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(lasso_oof_final[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "lasso_final_pred = np.expm1(lasso_oof_final)\n",
    "lasso_final_test = np.expm1(lasso_test_final)\n",
    "\n",
    "overall_lasso = smape(y_train, lasso_final_pred)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ LASSO FINAL SMAPE: {overall_lasso:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7efe853d-480b-472d-a9dc-76168000476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING LIGHTGBM WITH ULTIMATE FEATURES\n",
      "======================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[300]\tvalid_0's l1: 0.579076\n",
      "[600]\tvalid_0's l1: 0.564351\n",
      "[900]\tvalid_0's l1: 0.557212\n",
      "[1200]\tvalid_0's l1: 0.552434\n",
      "[1500]\tvalid_0's l1: 0.54972\n",
      "[1800]\tvalid_0's l1: 0.547357\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's l1: 0.545995\n",
      "Fold 1 SMAPE: 54.70%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[300]\tvalid_0's l1: 0.570868\n",
      "[600]\tvalid_0's l1: 0.555782\n",
      "[900]\tvalid_0's l1: 0.549249\n",
      "[1200]\tvalid_0's l1: 0.544366\n",
      "[1500]\tvalid_0's l1: 0.541279\n",
      "[1800]\tvalid_0's l1: 0.538929\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 0.538043\n",
      "Fold 2 SMAPE: 53.96%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[300]\tvalid_0's l1: 0.570121\n",
      "[600]\tvalid_0's l1: 0.555698\n",
      "[900]\tvalid_0's l1: 0.548804\n",
      "[1200]\tvalid_0's l1: 0.544692\n",
      "[1500]\tvalid_0's l1: 0.541812\n",
      "[1800]\tvalid_0's l1: 0.539525\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 0.538638\n",
      "Fold 3 SMAPE: 54.18%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[300]\tvalid_0's l1: 0.561788\n",
      "[600]\tvalid_0's l1: 0.548813\n",
      "[900]\tvalid_0's l1: 0.542692\n",
      "[1200]\tvalid_0's l1: 0.53863\n",
      "[1500]\tvalid_0's l1: 0.536069\n",
      "[1800]\tvalid_0's l1: 0.533886\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's l1: 0.532647\n",
      "Fold 4 SMAPE: 53.49%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[300]\tvalid_0's l1: 0.574432\n",
      "[600]\tvalid_0's l1: 0.559819\n",
      "[900]\tvalid_0's l1: 0.553711\n",
      "[1200]\tvalid_0's l1: 0.549977\n",
      "[1500]\tvalid_0's l1: 0.546694\n",
      "[1800]\tvalid_0's l1: 0.544727\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's l1: 0.543776\n",
      "Fold 5 SMAPE: 54.37%\n",
      "\n",
      "======================================================================\n",
      "âœ“ LIGHTGBM FINAL SMAPE: 54.14%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING LIGHTGBM WITH ULTIMATE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lgb_oof_final = np.zeros(len(X_train_ultimate_scaled))\n",
    "lgb_test_final = np.zeros(len(X_test_ultimate_scaled))\n",
    "\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'lambda_l1': 1.0,\n",
    "    'lambda_l2': 1.0,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_ultimate_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_tr, X_val = X_train_ultimate_scaled[train_idx], X_train_ultimate_scaled[val_idx]\n",
    "    y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params_final,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=150),\n",
    "            lgb.log_evaluation(300)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lgb_oof_final[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    lgb_test_final += model.predict(X_test_ultimate_scaled, num_iteration=model.best_iteration) / n_splits\n",
    "    \n",
    "    val_smape = smape(np.expm1(y_val), np.expm1(lgb_oof_final[val_idx]))\n",
    "    print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "lgb_final_pred = np.expm1(lgb_oof_final)\n",
    "lgb_final_test = np.expm1(lgb_test_final)\n",
    "\n",
    "overall_lgb = smape(y_train, lgb_final_pred)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ LIGHTGBM FINAL SMAPE: {overall_lgb:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5094a225-ef21-47b5-b28b-ab581601f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "Ridge                SMAPE: 59.55%\n",
      "Lasso                SMAPE: 66.42%\n",
      "LightGBM             SMAPE: 54.14%\n",
      "======================================================================\n",
      "ðŸ† BEST MODEL: LightGBM with SMAPE: 54.14%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_results = {\n",
    "    'Ridge': overall_ridge,\n",
    "    'Lasso': overall_lasso,\n",
    "    'LightGBM': overall_lgb\n",
    "}\n",
    "\n",
    "for name, score in final_results.items():\n",
    "    print(f\"{name:20s} SMAPE: {score:.2f}%\")\n",
    "\n",
    "best_model = min(final_results, key=final_results.get)\n",
    "best_score = final_results[best_model]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ† BEST MODEL: {best_model} with SMAPE: {best_score:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60cd3a35-f14f-4267-b6fb-7f492d1d86f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING MULTIPLE LIGHTGBM VARIANTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Training lgb_deep...\n",
      "======================================================================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.568431\n",
      "[1000]\tvalid_0's l1: 0.553175\n",
      "[1500]\tvalid_0's l1: 0.547037\n",
      "[2000]\tvalid_0's l1: 0.543623\n",
      "[2500]\tvalid_0's l1: 0.541191\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.541191\n",
      "Fold 1 SMAPE: 54.26%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.559367\n",
      "[1000]\tvalid_0's l1: 0.545257\n",
      "[1500]\tvalid_0's l1: 0.538974\n",
      "[2000]\tvalid_0's l1: 0.535707\n",
      "[2500]\tvalid_0's l1: 0.533498\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2499]\tvalid_0's l1: 0.533497\n",
      "Fold 2 SMAPE: 53.56%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.560234\n",
      "[1000]\tvalid_0's l1: 0.546384\n",
      "[1500]\tvalid_0's l1: 0.539944\n",
      "[2000]\tvalid_0's l1: 0.536521\n",
      "[2500]\tvalid_0's l1: 0.534487\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.534487\n",
      "Fold 3 SMAPE: 53.85%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.551738\n",
      "[1000]\tvalid_0's l1: 0.53826\n",
      "[1500]\tvalid_0's l1: 0.532315\n",
      "[2000]\tvalid_0's l1: 0.528692\n",
      "[2500]\tvalid_0's l1: 0.526745\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.526745\n",
      "Fold 4 SMAPE: 52.96%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.564023\n",
      "[1000]\tvalid_0's l1: 0.55013\n",
      "[1500]\tvalid_0's l1: 0.544291\n",
      "[2000]\tvalid_0's l1: 0.541053\n",
      "[2500]\tvalid_0's l1: 0.538976\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.538976\n",
      "Fold 5 SMAPE: 53.97%\n",
      "âœ“ lgb_deep Overall SMAPE: 53.72%\n",
      "\n",
      "======================================================================\n",
      "Training lgb_shallow...\n",
      "======================================================================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.568205\n",
      "[1000]\tvalid_0's l1: 0.557829\n",
      "[1500]\tvalid_0's l1: 0.551964\n",
      "[2000]\tvalid_0's l1: 0.548387\n",
      "[2500]\tvalid_0's l1: 0.545955\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2497]\tvalid_0's l1: 0.545939\n",
      "Fold 1 SMAPE: 54.67%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.56069\n",
      "[1000]\tvalid_0's l1: 0.550374\n",
      "[1500]\tvalid_0's l1: 0.544991\n",
      "[2000]\tvalid_0's l1: 0.541475\n",
      "[2500]\tvalid_0's l1: 0.53915\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2499]\tvalid_0's l1: 0.539141\n",
      "Fold 2 SMAPE: 54.05%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.561307\n",
      "[1000]\tvalid_0's l1: 0.55051\n",
      "[1500]\tvalid_0's l1: 0.544201\n",
      "[2000]\tvalid_0's l1: 0.541132\n",
      "[2500]\tvalid_0's l1: 0.539089\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2489]\tvalid_0's l1: 0.539052\n",
      "Fold 3 SMAPE: 54.25%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.553101\n",
      "[1000]\tvalid_0's l1: 0.543953\n",
      "[1500]\tvalid_0's l1: 0.539307\n",
      "[2000]\tvalid_0's l1: 0.536375\n",
      "[2500]\tvalid_0's l1: 0.534561\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2496]\tvalid_0's l1: 0.534544\n",
      "Fold 4 SMAPE: 53.64%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.56502\n",
      "[1000]\tvalid_0's l1: 0.554815\n",
      "[1500]\tvalid_0's l1: 0.550692\n",
      "[2000]\tvalid_0's l1: 0.547504\n",
      "[2500]\tvalid_0's l1: 0.545494\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2499]\tvalid_0's l1: 0.545493\n",
      "Fold 5 SMAPE: 54.50%\n",
      "âœ“ lgb_shallow Overall SMAPE: 54.22%\n",
      "\n",
      "======================================================================\n",
      "Training lgb_balanced...\n",
      "======================================================================\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.564615\n",
      "[1000]\tvalid_0's l1: 0.55385\n",
      "[1500]\tvalid_0's l1: 0.548542\n",
      "[2000]\tvalid_0's l1: 0.545367\n",
      "[2500]\tvalid_0's l1: 0.54364\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.54364\n",
      "Fold 1 SMAPE: 54.47%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.555908\n",
      "[1000]\tvalid_0's l1: 0.544282\n",
      "[1500]\tvalid_0's l1: 0.538665\n",
      "[2000]\tvalid_0's l1: 0.535862\n",
      "[2500]\tvalid_0's l1: 0.534244\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's l1: 0.534244\n",
      "Fold 2 SMAPE: 53.61%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.556505\n",
      "[1000]\tvalid_0's l1: 0.545293\n",
      "[1500]\tvalid_0's l1: 0.540363\n",
      "[2000]\tvalid_0's l1: 0.537537\n",
      "[2500]\tvalid_0's l1: 0.535649\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2498]\tvalid_0's l1: 0.535636\n",
      "Fold 3 SMAPE: 53.95%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.548624\n",
      "[1000]\tvalid_0's l1: 0.538547\n",
      "[1500]\tvalid_0's l1: 0.534137\n",
      "[2000]\tvalid_0's l1: 0.531381\n",
      "[2500]\tvalid_0's l1: 0.52966\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2495]\tvalid_0's l1: 0.529652\n",
      "Fold 4 SMAPE: 53.20%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\tvalid_0's l1: 0.561381\n",
      "[1000]\tvalid_0's l1: 0.550119\n",
      "[1500]\tvalid_0's l1: 0.545134\n",
      "[2000]\tvalid_0's l1: 0.542523\n",
      "[2500]\tvalid_0's l1: 0.540725\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2499]\tvalid_0's l1: 0.540724\n",
      "Fold 5 SMAPE: 54.07%\n",
      "âœ“ lgb_balanced Overall SMAPE: 53.86%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MULTIPLE LIGHTGBM VARIANTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lgb_variants = {}\n",
    "lgb_test_variants = {}\n",
    "\n",
    "# Variant 1: Deep trees, low learning rate\n",
    "params_v1 = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 128,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 15,\n",
    "    'lambda_l1': 2.0,\n",
    "    'lambda_l2': 2.0,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Variant 2: Shallow trees, higher learning rate\n",
    "params_v2 = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 48,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 3,\n",
    "    'min_child_samples': 30,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 0.5,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 123\n",
    "}\n",
    "\n",
    "# Variant 3: Balanced\n",
    "params_v3 = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 96,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 4,\n",
    "    'min_child_samples': 20,\n",
    "    'lambda_l1': 1.0,\n",
    "    'lambda_l2': 1.0,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 999\n",
    "}\n",
    "\n",
    "variants = [\n",
    "    ('lgb_deep', params_v1),\n",
    "    ('lgb_shallow', params_v2),\n",
    "    ('lgb_balanced', params_v3)\n",
    "]\n",
    "\n",
    "for variant_name, params in variants:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {variant_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    oof = np.zeros(len(X_train_ultimate_scaled))\n",
    "    test_pred = np.zeros(len(X_test_ultimate_scaled))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_ultimate_scaled)):\n",
    "        X_tr, X_val = X_train_ultimate_scaled[train_idx], X_train_ultimate_scaled[val_idx]\n",
    "        y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=2500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(500)]\n",
    "        )\n",
    "        \n",
    "        oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        test_pred += model.predict(X_test_ultimate_scaled, num_iteration=model.best_iteration) / n_splits\n",
    "        \n",
    "        val_smape = smape(np.expm1(y_val), np.expm1(oof[val_idx]))\n",
    "        print(f\"Fold {fold+1} SMAPE: {val_smape:.2f}%\")\n",
    "    \n",
    "    lgb_variants[variant_name] = np.expm1(oof)\n",
    "    lgb_test_variants[variant_name] = np.expm1(test_pred)\n",
    "    \n",
    "    score = smape(y_train, lgb_variants[variant_name])\n",
    "    print(f\"âœ“ {variant_name} Overall SMAPE: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f17e8b9-464f-43f5-892f-bda79fa36778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING SUPER ENSEMBLE\n",
      "======================================================================\n",
      "\n",
      "Individual Model Scores:\n",
      "  ridge               : 59.55%\n",
      "  lasso               : 66.42%\n",
      "  lgbm                : 54.14%\n",
      "  lgb_deep            : 53.72%\n",
      "  lgb_shallow         : 54.22%\n",
      "  lgb_balanced        : 53.86%\n",
      "\n",
      "Optimizing weights for 6 models...\n",
      "\n",
      "======================================================================\n",
      "OPTIMAL ENSEMBLE WEIGHTS:\n",
      "  ridge               : 0.0000 (0.0%)\n",
      "  lasso               : 0.0000 (0.0%)\n",
      "  lgbm                : 0.2508 (25.1%)\n",
      "  lgb_deep            : 0.0953 (9.5%)\n",
      "  lgb_shallow         : 0.2809 (28.1%)\n",
      "  lgb_balanced        : 0.3730 (37.3%)\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FINAL ENSEMBLE SMAPE: 53.65%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SUPER ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect all OOF predictions\n",
    "all_oof = {\n",
    "    'ridge': ridge_final_pred,\n",
    "    'lasso': lasso_final_pred,\n",
    "    'lgbm': lgb_final_pred,\n",
    "    **lgb_variants\n",
    "}\n",
    "\n",
    "# Collect all test predictions\n",
    "all_test = {\n",
    "    'ridge': ridge_final_test,\n",
    "    'lasso': lasso_final_test,\n",
    "    'lgbm': lgb_final_test,\n",
    "    **lgb_test_variants\n",
    "}\n",
    "\n",
    "# Show individual scores\n",
    "print(\"\\nIndividual Model Scores:\")\n",
    "for name, pred in all_oof.items():\n",
    "    score = smape(y_train, pred)\n",
    "    print(f\"  {name:20s}: {score:.2f}%\")\n",
    "\n",
    "# Optimize ensemble weights\n",
    "def ensemble_objective(weights, preds_dict, y_true):\n",
    "    weights = np.abs(weights)\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    ensemble = np.zeros(len(y_true))\n",
    "    for i, pred in enumerate(preds_dict.values()):\n",
    "        ensemble += weights[i] * pred\n",
    "    \n",
    "    return smape(y_true, ensemble)\n",
    "\n",
    "model_names = list(all_oof.keys())\n",
    "initial_weights = np.ones(len(model_names)) / len(model_names)\n",
    "\n",
    "print(f\"\\nOptimizing weights for {len(model_names)} models...\")\n",
    "result = minimize(\n",
    "    ensemble_objective,\n",
    "    initial_weights,\n",
    "    args=(all_oof, y_train),\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 2000}\n",
    ")\n",
    "\n",
    "optimal_weights = np.abs(result.x)\n",
    "optimal_weights /= optimal_weights.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMAL ENSEMBLE WEIGHTS:\")\n",
    "for name, weight in zip(model_names, optimal_weights):\n",
    "    print(f\"  {name:20s}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "\n",
    "# Create final ensemble\n",
    "final_ensemble_oof = np.zeros(len(y_train))\n",
    "final_ensemble_test = np.zeros(len(X_test_ultimate_scaled))\n",
    "\n",
    "for i, (name, pred) in enumerate(all_oof.items()):\n",
    "    final_ensemble_oof += optimal_weights[i] * pred\n",
    "    final_ensemble_test += optimal_weights[i] * all_test[name]\n",
    "\n",
    "final_smape = smape(y_train, final_ensemble_oof)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸŽ¯ FINAL ENSEMBLE SMAPE: {final_smape:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3563f65d-2c9e-42a1-b481-8988f2f9b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL SUBMISSION CREATED!\n",
      "======================================================================\n",
      "File: submission_final.csv\n",
      "Shape: (75000, 2)\n",
      "\n",
      "Prediction Stats:\n",
      "  Min:    1.01\n",
      "  Max:    222.71\n",
      "  Mean:   17.88\n",
      "  Median: 13.94\n",
      "\n",
      "======================================================================\n",
      "ðŸ† FINAL CROSS-VALIDATION SMAPE: 53.65%\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Post-process predictions\n",
    "final_ensemble_test_processed = postprocess_predictions(final_ensemble_test, train['price'])\n",
    "\n",
    "# Create submission\n",
    "submission_final = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'],\n",
    "    'price': final_ensemble_test_processed.round(2)\n",
    "})\n",
    "\n",
    "submission_final.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUBMISSION CREATED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"File: submission_final.csv\")\n",
    "print(f\"Shape: {submission_final.shape}\")\n",
    "print(f\"\\nPrediction Stats:\")\n",
    "print(f\"  Min:    {submission_final['price'].min():.2f}\")\n",
    "print(f\"  Max:    {submission_final['price'].max():.2f}\")\n",
    "print(f\"  Mean:   {submission_final['price'].mean():.2f}\")\n",
    "print(f\"  Median: {submission_final['price'].median():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ† FINAL CROSS-VALIDATION SMAPE: {final_smape:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c09caa8-5b51-4592-8933-19f0a84bab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ† FINAL SUBMISSION FILE CREATED SUCCESSFULLY!\n",
      "======================================================================\n",
      "File saved as: submission.csv\n",
      "Total rows: 75000\n",
      "======================================================================\n",
      "\n",
      "Sample preview:\n",
      "       sample_id  price\n",
      "4020           1  22.07\n",
      "39491          3  26.01\n",
      "6852           9  15.30\n",
      "64597         19   6.23\n",
      "10141         20  21.33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# âœ… Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'].astype(int),\n",
    "    'price': final_ensemble_test_processed.astype(float).round(2)\n",
    "})\n",
    "\n",
    "# âœ… Sort and save\n",
    "submission = submission.sort_values(by='sample_id')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# âœ… Confirmation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ† FINAL SUBMISSION FILE CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"File saved as: submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# (Optional) Quick preview\n",
    "print(\"\\nSample preview:\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9df78-7d82-4954-86ed-31ed6865e591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
